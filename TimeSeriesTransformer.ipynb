{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a130a6",
   "metadata": {
    "id": "77a130a6"
   },
   "source": [
    "## Time Series Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349396b5",
   "metadata": {
    "id": "349396b5"
   },
   "source": [
    "https://huggingface.co/blog/time-series-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139f0c6",
   "metadata": {
    "id": "2139f0c6"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install accelerate\n",
    "# !pip install ujson\n",
    "# !pip install gluonts\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591305b4",
   "metadata": {
    "id": "591305b4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"monash_tsf\", \"tourism_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af277c",
   "metadata": {
    "id": "60af277c"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad99bc",
   "metadata": {
    "id": "d4ad99bc"
   },
   "outputs": [],
   "source": [
    "train_example = dataset['train'][0]\n",
    "train_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264bbc9",
   "metadata": {
    "id": "4264bbc9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_example['start'])\n",
    "print(train_example['target'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779d9dd",
   "metadata": {
    "id": "1779d9dd"
   },
   "outputs": [],
   "source": [
    "validation_example = dataset['validation'][0]\n",
    "validation_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fb1d9",
   "metadata": {
    "id": "ee3fb1d9"
   },
   "outputs": [],
   "source": [
    "len(train_example['target']), len(validation_example['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e361",
   "metadata": {
    "id": "3866e361"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"], color=\"blue\")\n",
    "axes.plot(validation_example[\"target\"], color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95466f7",
   "metadata": {
    "id": "b95466f7"
   },
   "outputs": [],
   "source": [
    "freq = \"1M\"\n",
    "prediction_length = 24\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e08ef",
   "metadata": {
    "id": "b72e08ef"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@lru_cache\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06708a9",
   "metadata": {
    "id": "d06708a9"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a14c00",
   "metadata": {
    "id": "d4a14c00"
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752fd67",
   "metadata": {
    "id": "1752fd67"
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "print(time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa4541",
   "metadata": {
    "id": "4bfa4541"
   },
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e0803",
   "metadata": {
    "id": "135e0803"
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c8d6a",
   "metadata": {
    "id": "110c8d6a"
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd13aa",
   "metadata": {
    "id": "c5dd13aa"
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in its life the value of the time series is,\n",
    "            # sort of a running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf967b5",
   "metadata": {
    "id": "daf967b5"
   },
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89826d56",
   "metadata": {
    "id": "89826d56"
   },
   "outputs": [],
   "source": [
    "from gluonts.itertools import Cyclic, IterableSlice, PseudoShuffled\n",
    "from gluonts.torch.util import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\") + SelectFields(TRAINING_INPUT_NAMES)\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    training_instances = instance_splitter.apply(\n",
    "        Cyclic(transformed_data)\n",
    "        if shuffle_buffer_length is None\n",
    "        else PseudoShuffled(\n",
    "            Cyclic(transformed_data),\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # from the training instances iterator we now return a Dataloader which will\n",
    "    # continue to sample random windows for as long as it is called\n",
    "    # to return batch_size of the appropriate tensors ready for training!\n",
    "    return IterableSlice(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                IterableDataset(training_instances),\n",
    "                batch_size=batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        ),\n",
    "        num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a1c79",
   "metadata": {
    "id": "a93a1c79"
   },
   "outputs": [],
   "source": [
    "def create_test_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"test\") + SelectFields(PREDICTION_INPUT_NAMES)\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    # This returns a Dataloader which will go over the dataset once.\n",
    "    return DataLoader(\n",
    "        IterableDataset(testing_instances), batch_size=batch_size, **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3c0bb",
   "metadata": {
    "id": "d3f3c0bb"
   },
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "test_dataloader = create_test_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7acfd",
   "metadata": {
    "id": "74f7acfd"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ee41c",
   "metadata": {
    "id": "fd0ee41c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# perform forward pass\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"]\n",
    "    if config.num_static_categorical_features > 0 else None,\n",
    "    static_real_features=batch[\"static_real_features\"]\n",
    "    if config.num_static_real_features > 0 else None,\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b921a5c",
   "metadata": {
    "id": "8b921a5c"
   },
   "outputs": [],
   "source": [
    "print(\"Loss:\", outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8533551",
   "metadata": {
    "id": "e8533551"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0 else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0 else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd849f18",
   "metadata": {
    "id": "fd849f18"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "forecasts = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0 else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0 else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11eaee",
   "metadata": {
    "id": "da11eaee"
   },
   "outputs": [],
   "source": [
    "forecasts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88214819",
   "metadata": {
    "id": "88214819"
   },
   "outputs": [],
   "source": [
    "forecasts = np.vstack(forecasts)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b83c4",
   "metadata": {
    "id": "228b83c4"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1)\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "for item_id, ts in enumerate(test_dataset):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id], \n",
    "        references=np.array(ground_truth), \n",
    "        training=np.array(training_data), \n",
    "        periodicity=get_seasonality(freq))\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "    \n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id], \n",
    "        references=np.array(ground_truth), \n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b3b8b",
   "metadata": {
    "id": "c91b3b8b"
   },
   "outputs": [],
   "source": [
    "print(f\"MASE: {np.mean(mase_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc836a7d",
   "metadata": {
    "id": "fc836a7d"
   },
   "outputs": [],
   "source": [
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab19490",
   "metadata": {
    "id": "eab19490"
   },
   "outputs": [],
   "source": [
    "plt.scatter(mase_metrics, smape_metrics, alpha=0.3)\n",
    "plt.xlabel(\"MASE\")\n",
    "plt.ylabel(\"sMAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4eff55",
   "metadata": {
    "id": "ef4eff55"
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "def plot(ts_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=test_dataset[ts_index][FieldName.START],\n",
    "        periods=len(test_dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    # Major ticks every half year, minor ticks every month,\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "    ax.plot(\n",
    "        index[-2*prediction_length:], \n",
    "        test_dataset[ts_index][\"target\"][-2*prediction_length:],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:], \n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "    \n",
    "    plt.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0), \n",
    "        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0), \n",
    "        alpha=0.3, \n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c7c69",
   "metadata": {
    "id": "ed4c7c69"
   },
   "outputs": [],
   "source": [
    "plot(334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f697ac3",
   "metadata": {
    "id": "7f697ac3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 - Deep Learning",
   "language": "python",
   "name": "deep_learning_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
